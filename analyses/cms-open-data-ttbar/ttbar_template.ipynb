{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dba380f",
   "metadata": {},
   "source": [
    "# CMS Open Data $t\\bar{t}$: from data delivery to statistical inference\n",
    "\n",
    "We are using [2015 CMS Open Data](https://cms.cern/news/first-cms-open-data-lhc-run-2-released) in this demonstration to showcase an analysis pipeline.\n",
    "It features data delivery and processing, histogram construction and visualization, as well as statistical inference.\n",
    "\n",
    "This notebook was developed in the context of the [IRIS-HEP AGC tools 2022 workshop](https://indico.cern.ch/e/agc-tools-2).\n",
    "This work was supported by the U.S. National Science Foundation (NSF) Cooperative Agreement OAC-1836650 (IRIS-HEP).\n",
    "\n",
    "This is a **technical demonstration**.\n",
    "We are including the relevant workflow aspects that physicists need in their work, but we are not focusing on making every piece of the demonstration physically meaningful.\n",
    "This concerns in particular systematic uncertainties: we capture the workflow, but the actual implementations are more complex in practice.\n",
    "If you are interested in the physics side of analyzing top pair production, check out the latest results from [ATLAS](https://twiki.cern.ch/twiki/bin/view/AtlasPublic/TopPublicResults) and [CMS](https://cms-results.web.cern.ch/cms-results/public-results/preliminary-results/)!\n",
    "If you would like to see more technical demonstrations, also check out an [ATLAS Open Data example](https://indico.cern.ch/event/1076231/contributions/4560405/) demonstrated previously.\n",
    "\n",
    "This notebook implements most of the analysis pipeline shown in the following picture, using the tools also mentioned there:\n",
    "![ecosystem visualization](utils/ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c1def",
   "metadata": {},
   "source": [
    "### Data pipelines\n",
    "\n",
    "There are two possible pipelines: one with `ServiceX` enabled, and one using only `coffea` for processing.\n",
    "![processing pipelines](utils/processing_pipelines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3768331",
   "metadata": {},
   "source": [
    "### Imports: setting up our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f704d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "import cabinetry\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import transforms\n",
    "from coffea.nanoevents.methods import base, vector\n",
    "from coffea.nanoevents.schemas.base import BaseSchema, zip_forms\n",
    "from servicex import ServiceXDataset\n",
    "from func_adl import ObjectStream\n",
    "from func_adl_servicex import ServiceXSourceUpROOT\n",
    "import hist\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "\n",
    "import utils  # contains code for bookkeeping and cosmetics, as well as some boilerplate\n",
    "\n",
    "logging.getLogger(\"cabinetry\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e277e0d4",
   "metadata": {},
   "source": [
    "### Configuration: number of files and data delivery path\n",
    "\n",
    "The number of files per sample set here determines the size of the dataset we are processing.\n",
    "There are 9 samples being used here, all part of the 2015 CMS Open Data release.\n",
    "They are pre-converted from miniAOD files into ntuple format, similar to nanoAODs.\n",
    "More details about the inputs can be found [here](https://github.com/iris-hep/analysis-grand-challenge/tree/main/datasets/cms-open-data-2015).\n",
    "\n",
    "The table below summarizes the amount of data processed depending on the `N_FILES_MAX_PER_SAMPLE` setting.\n",
    "\n",
    "| setting | number of files | total size |\n",
    "| --- | --- | --- |\n",
    "| `1` | 9 | 16.3 GB |\n",
    "| `5` | 45 | 81.7 GB |\n",
    "| `10` | 86 | 157 GB |\n",
    "| `50` | 357 | 678 GB |\n",
    "| `100` | 590 | 1.09 TB |\n",
    "| `500` | 1542 | 2.58 TB |\n",
    "| `1000` | 2249 | 3.57 TB |\n",
    "| `-1` | 2269 | 3.59 TB |\n",
    "\n",
    "The input files are all in the 1â€“2 GB range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a3550",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBAL CONFIGURATION\n",
    "\n",
    "# input datasets list in json format\n",
    "DATASETS = \"_DATASETS_\"\n",
    "\n",
    "# input files per process, set to e.g. 10 (smaller number = faster)\n",
    "N_FILES_MAX_PER_SAMPLE = _NFILES_\n",
    "\n",
    "# enable Dask\n",
    "USE_DASK = False\n",
    "\n",
    "# enable ServiceX\n",
    "USE_SERVICEX = False\n",
    "\n",
    "# ServiceX: ignore cache with repeated queries\n",
    "SERVICEX_IGNORE_CACHE = False\n",
    "\n",
    "# analysis facility: set to \"coffea_casa\" for coffea-casa environments, \"EAF\" for FNAL, \"local\" for local setups\n",
    "AF = \"coffea_casa\"\n",
    "\n",
    "\n",
    "### BENCHMARKING-SPECIFIC SETTINGS\n",
    "\n",
    "# chunk size to use\n",
    "CHUNKSIZE = 500_000\n",
    "\n",
    "# metadata to propagate through to metrics\n",
    "AF_NAME = \"_AFNAME_\"  # \"ssl-dev\" allows for the switch to local data on /data\n",
    "SYSTEMATICS = \"all\"  # currently has no effect\n",
    "CORES_PER_WORKER = 2  # does not do anything, only used for metric gathering (set to 2 for distributed coffea-casa)\n",
    "\n",
    "# scaling for local setups with FuturesExecutor\n",
    "NUM_CORES = _WORKERS_\n",
    "\n",
    "# only I/O, all other processing disabled\n",
    "DISABLE_PROCESSING = _NOPROC_\n",
    "\n",
    "# read additional branches (only with DISABLE_PROCESSING = True)\n",
    "# acceptable values are 4, 15, 25, 50 (corresponding to % of file read), 4% corresponds to the standard branches used in the notebook\n",
    "IO_FILE_PERCENT = _FRACTION_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ff5bc",
   "metadata": {},
   "source": [
    "### Defining our `coffea` Processor\n",
    "\n",
    "The processor includes a lot of the physics analysis details:\n",
    "- event filtering and the calculation of observables,\n",
    "- event weighting,\n",
    "- calculating systematic uncertainties at the event and object level,\n",
    "- filling all the information into histograms that get aggregated and ultimately returned to us by `coffea`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c0e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functions creating systematic variations\n",
    "def flat_variation(ones):\n",
    "    # 2.5% weight variations\n",
    "    return (1.0 + np.array([0.025, -0.025], dtype=np.float32)) * ones[:, None]\n",
    "\n",
    "\n",
    "def btag_weight_variation(i_jet, jet_pt):\n",
    "    # weight variation depending on i-th jet pT (7.5% as default value, multiplied by i-th jet pT / 50 GeV)\n",
    "    return 1 + np.array([0.075, -0.075]) * (ak.singletons(jet_pt[:, i_jet]) / 50).to_numpy()\n",
    "\n",
    "\n",
    "def jet_pt_resolution(pt):\n",
    "    # normal distribution with 5% variations, shape matches jets\n",
    "    counts = ak.num(pt)\n",
    "    pt_flat = ak.flatten(pt)\n",
    "    resolution_variation = np.random.normal(np.ones_like(pt_flat), 0.05)\n",
    "    return ak.unflatten(resolution_variation, counts)\n",
    "\n",
    "\n",
    "class TtbarAnalysis(processor.ProcessorABC):\n",
    "    def __init__(self, disable_processing, io_file_percent):\n",
    "        num_bins = 25\n",
    "        bin_low = 50\n",
    "        bin_high = 550\n",
    "        name = \"observable\"\n",
    "        label = \"observable [GeV]\"\n",
    "        self.hist = (\n",
    "            hist.Hist.new.Reg(num_bins, bin_low, bin_high, name=name, label=label)\n",
    "            .StrCat([\"4j1b\", \"4j2b\"], name=\"region\", label=\"Region\")\n",
    "            .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "            .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "            .Weight()\n",
    "        )\n",
    "        self.disable_processing = disable_processing\n",
    "        self.io_file_percent = io_file_percent\n",
    "\n",
    "    def only_do_IO(self, events):\n",
    "        # standard AGC branches cover 4% of the data\n",
    "            branches_to_read = [\"jet_pt\", \"jet_eta\", \"jet_phi\", \"jet_btag\", \"jet_e\", \"muon_pt\", \"electron_pt\"]\n",
    "            if self.io_file_percent not in [4, 15, 25, 50]:\n",
    "                raise NotImplementedError(\"supported values for I/O percentage are 4, 15, 25, 50\")\n",
    "            if self.io_file_percent >= 15:\n",
    "                branches_to_read += [\"trigobj_e\"]\n",
    "            if self.io_file_percent >= 25:\n",
    "                branches_to_read += [\"trigobj_pt\"]\n",
    "            if self.io_file_percent >= 50:\n",
    "                branches_to_read += [\"trigobj_eta\", \"trigobj_phi\", \"jet_px\", \"jet_py\", \"jet_pz\", \"jet_ch\"]\n",
    "\n",
    "            for branch in branches_to_read:\n",
    "                if \"_\" in branch:\n",
    "                    object_type, property_name = branch.split(\"_\")\n",
    "                    if property_name == \"e\":\n",
    "                        property_name = \"energy\"\n",
    "                    ak.materialized(events[object_type][property_name])\n",
    "                else:\n",
    "                    ak.materialized(events[branch])\n",
    "            return {\"hist\": {}}\n",
    "\n",
    "    def process(self, events):\n",
    "        if self.disable_processing:\n",
    "            # IO testing with no subsequent processing\n",
    "            return self.only_do_IO(events)\n",
    "\n",
    "        histogram = self.hist.copy()\n",
    "\n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "        variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "\n",
    "        # normalization for MC\n",
    "        x_sec = events.metadata[\"xsec\"]\n",
    "        nevts_total = events.metadata[\"nevts\"]\n",
    "        lumi = 3378 # /pb\n",
    "        if process != \"data\":\n",
    "            xsec_weight = x_sec * lumi / nevts_total\n",
    "        else:\n",
    "            xsec_weight = 1\n",
    "\n",
    "        #### systematics\n",
    "        # example of a simple flat weight variation, using the coffea nanoevents systematics feature\n",
    "        if process == \"wjets\":\n",
    "            events.add_systematic(\"scale_var\", \"UpDownSystematic\", \"weight\", flat_variation)\n",
    "\n",
    "        # jet energy scale / resolution systematics\n",
    "        # need to adjust schema to instead use coffea add_systematic feature, especially for ServiceX\n",
    "        # cannot attach pT variations to events.jet, so attach to events directly\n",
    "        # and subsequently scale pT by these scale factors\n",
    "        events[\"pt_nominal\"] = 1.0\n",
    "        events[\"pt_scale_up\"] = 1.03\n",
    "        events[\"pt_res_up\"] = jet_pt_resolution(events.jet.pt)\n",
    "\n",
    "        pt_variations = [\"pt_nominal\", \"pt_scale_up\", \"pt_res_up\"] if variation == \"nominal\" else [\"pt_nominal\"]\n",
    "        for pt_var in pt_variations:\n",
    "\n",
    "            ### event selection\n",
    "            # very very loosely based on https://arxiv.org/abs/2006.13076\n",
    "\n",
    "            # pT > 25 GeV for leptons & jets\n",
    "            selected_electrons = events.electron[events.electron.pt > 25]\n",
    "            selected_muons = events.muon[events.muon.pt > 25]\n",
    "            jet_filter = events.jet.pt * events[pt_var] > 25  # pT > 25 GeV for jets (scaled by systematic variations)\n",
    "            selected_jets = events.jet[jet_filter]\n",
    "\n",
    "            # single lepton requirement\n",
    "            event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "            # at least four jets\n",
    "            pt_var_modifier = events[pt_var] if \"res\" not in pt_var else events[pt_var][jet_filter]\n",
    "            event_filters = event_filters & (ak.count(selected_jets.pt * pt_var_modifier, axis=1) >= 4)\n",
    "            # at least one b-tagged jet (\"tag\" means score above threshold)\n",
    "            B_TAG_THRESHOLD = 0.5\n",
    "            event_filters = event_filters & (ak.sum(selected_jets.btag >= B_TAG_THRESHOLD, axis=1) >= 1)\n",
    "\n",
    "            # apply event filters\n",
    "            selected_events = events[event_filters]\n",
    "            selected_electrons = selected_electrons[event_filters]\n",
    "            selected_muons = selected_muons[event_filters]\n",
    "            selected_jets = selected_jets[event_filters]\n",
    "\n",
    "            for region in [\"4j1b\", \"4j2b\"]:\n",
    "                # further filtering: 4j1b CR with single b-tag, 4j2b SR with two or more tags\n",
    "                if region == \"4j1b\":\n",
    "                    region_filter = ak.sum(selected_jets.btag >= B_TAG_THRESHOLD, axis=1) == 1\n",
    "                    selected_jets_region = selected_jets[region_filter]\n",
    "                    # use HT (scalar sum of jet pT) as observable\n",
    "                    pt_var_modifier = (\n",
    "                        events[event_filters][region_filter][pt_var]\n",
    "                        if \"res\" not in pt_var\n",
    "                        else events[pt_var][jet_filter][event_filters][region_filter]\n",
    "                    )\n",
    "                    observable = ak.sum(selected_jets_region.pt * pt_var_modifier, axis=-1)\n",
    "\n",
    "                elif region == \"4j2b\":\n",
    "                    region_filter = ak.sum(selected_jets.btag > B_TAG_THRESHOLD, axis=1) >= 2\n",
    "                    selected_jets_region = selected_jets[region_filter]\n",
    "\n",
    "                    # reconstruct hadronic top as bjj system with largest pT\n",
    "                    # the jet energy scale / resolution effect is not propagated to this observable at the moment\n",
    "                    trijet = ak.combinations(selected_jets_region, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "                    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # calculate four-momentum of tri-jet system\n",
    "                    trijet[\"max_btag\"] = np.maximum(trijet.j1.btag, np.maximum(trijet.j2.btag, trijet.j3.btag))\n",
    "                    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "                    # pick trijet candidate with largest pT and calculate mass of system\n",
    "                    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "                    observable = ak.flatten(trijet_mass)\n",
    "\n",
    "                ### histogram filling\n",
    "                if pt_var == \"pt_nominal\":\n",
    "                    # nominal pT, but including 2-point systematics\n",
    "                    histogram.fill(\n",
    "                            observable=observable, region=region, process=process,\n",
    "                            variation=variation, weight=xsec_weight\n",
    "                        )\n",
    "\n",
    "                    if variation == \"nominal\":\n",
    "                        # also fill weight-based variations for all nominal samples\n",
    "                        for weight_name in events.systematics.fields:\n",
    "                            for direction in [\"up\", \"down\"]:\n",
    "                                # extract the weight variations and apply all event & region filters\n",
    "                                weight_variation = events.systematics[weight_name][direction][\n",
    "                                    f\"weight_{weight_name}\"][event_filters][region_filter]\n",
    "                                # fill histograms\n",
    "                                histogram.fill(\n",
    "                                    observable=observable, region=region, process=process,\n",
    "                                    variation=f\"{weight_name}_{direction}\", weight=xsec_weight*weight_variation\n",
    "                                )\n",
    "\n",
    "                        # calculate additional systematics: b-tagging variations\n",
    "                        for i_var, weight_name in enumerate([f\"btag_var_{i}\" for i in range(4)]):\n",
    "                            for i_dir, direction in enumerate([\"up\", \"down\"]):\n",
    "                                # create systematic variations that depend on object properties (here: jet pT)\n",
    "                                if len(observable):\n",
    "                                    weight_variation = btag_weight_variation(i_var, selected_jets_region.pt)[:, i_dir]\n",
    "                                else:\n",
    "                                    weight_variation = 1 # no events selected\n",
    "                                histogram.fill(\n",
    "                                    observable=observable, region=region, process=process,\n",
    "                                    variation=f\"{weight_name}_{direction}\", weight=xsec_weight*weight_variation\n",
    "                                )\n",
    "\n",
    "                elif variation == \"nominal\":\n",
    "                    # pT variations for nominal samples\n",
    "                    histogram.fill(\n",
    "                            observable=observable, region=region, process=process,\n",
    "                            variation=pt_var, weight=xsec_weight\n",
    "                        )\n",
    "\n",
    "        output = {\"nevents\": {events.metadata[\"dataset\"]: len(events)}, \"hist\": histogram}\n",
    "\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa352f9",
   "metadata": {},
   "source": [
    "### AGC `coffea` schema\n",
    "\n",
    "When using `coffea`, we can benefit from the schema functionality to group columns into convenient objects.\n",
    "This schema is taken from [mat-adamec/agc_coffea](https://github.com/mat-adamec/agc_coffea)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09ff19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AGCSchema(BaseSchema):\n",
    "    def __init__(self, base_form):\n",
    "        super().__init__(base_form)\n",
    "        self._form[\"contents\"] = self._build_collections(self._form[\"contents\"])\n",
    "\n",
    "    def _build_collections(self, branch_forms):\n",
    "        names = set([k.split('_')[0] for k in branch_forms.keys() if not (k.startswith('number'))])\n",
    "        # Remove n(names) from consideration. It's safe to just remove names that start with n, as nothing else begins with n in our fields.\n",
    "        # Also remove GenPart, PV and MET because they deviate from the pattern of having a 'number' field.\n",
    "        names = [k for k in names if not (k.startswith('n') | k.startswith('met') | k.startswith('GenPart') | k.startswith('PV'))]\n",
    "        output = {}\n",
    "        for name in names:\n",
    "            offsets = transforms.counts2offsets_form(branch_forms['number' + name])\n",
    "            content = {k[len(name)+1:]: branch_forms[k] for k in branch_forms if (k.startswith(name + \"_\") & (k[len(name)+1:] != 'e'))}\n",
    "            # Add energy separately so its treated correctly by the p4 vector.\n",
    "            content['energy'] = branch_forms[name+'_e']\n",
    "            # Check for LorentzVector\n",
    "            output[name] = zip_forms(content, name, 'PtEtaPhiELorentzVector', offsets=offsets)\n",
    "\n",
    "        # Handle GenPart, PV, MET. Note that all the nPV_*'s should be the same. We just use one.\n",
    "        #output['met'] = zip_forms({k[len('met')+1:]: branch_forms[k] for k in branch_forms if k.startswith('met_')}, 'met')\n",
    "        #output['GenPart'] = zip_forms({k[len('GenPart')+1:]: branch_forms[k] for k in branch_forms if k.startswith('GenPart_')}, 'GenPart', offsets=transforms.counts2offsets_form(branch_forms['numGenPart']))\n",
    "        #output['PV'] = zip_forms({k[len('PV')+1:]: branch_forms[k] for k in branch_forms if (k.startswith('PV_') & ('npvs' not in k))}, 'PV', offsets=transforms.counts2offsets_form(branch_forms['nPV_x']))\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def behavior(self):\n",
    "        behavior = {}\n",
    "        behavior.update(base.behavior)\n",
    "        behavior.update(vector.behavior)\n",
    "        return behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba0f32",
   "metadata": {},
   "source": [
    "### \"Fileset\" construction and metadata\n",
    "\n",
    "Here, we gather all the required information about the files we want to process: paths to the files and asociated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37c160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fileset = utils.construct_fileset(N_FILES_MAX_PER_SAMPLE, use_xcache=False, af_name=AF_NAME, datasets=DATASETS)  # local files on /data for ssl-dev\n",
    "\n",
    "print(f\"processes in fileset: {list(fileset.keys())}\")\n",
    "print(f\"\\nexample of information in fileset:\\n{{\\n  'files': [{fileset['ttbar__nominal']['files'][0]}, ...],\")\n",
    "print(f\"  'metadata': {fileset['ttbar__nominal']['metadata']}\\n}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d1e105",
   "metadata": {},
   "source": [
    "### ServiceX-specific functionality: query setup\n",
    "\n",
    "Define the func_adl query to be used for the purpose of extracting columns and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(source: ObjectStream) -> ObjectStream:\n",
    "    \"\"\"Query for event / column selection: >=4j >=1b, ==1 lep with pT>25 GeV, return relevant columns\n",
    "    \"\"\"\n",
    "    return source.Where(lambda e:\n",
    "        # == 1 lep\n",
    "        e.electron_pt.Where(lambda pT: pT > 25).Count() + e.muon_pt.Where(lambda pT: pT > 25).Count()== 1\n",
    "        )\\\n",
    "        .Where(lambda e:\\\n",
    "            # >= 4 jets\n",
    "            e.jet_pt.Where(lambda pT: pT > 25).Count() >= 4\n",
    "        )\\\n",
    "        .Where(lambda e:\\\n",
    "            # >= 1 jet with pT > 25 GeV and b-tag >= 0.5\n",
    "            {\"pT\": e.jet_pt, \"btag\": e.jet_btag}.Zip().Where(lambda jet: jet.btag >= 0.5 and jet.pT > 25).Count() >= 1\n",
    "        )\\\n",
    "        .Select(lambda e:\\\n",
    "            # return columns\n",
    "            {\n",
    "                \"electron_e\": e.electron_e,\n",
    "                \"electron_pt\": e.electron_pt,\n",
    "                \"muon_e\": e.muon_e,\n",
    "                \"muon_pt\": e.muon_pt,\n",
    "                \"jet_e\": e.jet_e,\n",
    "                \"jet_pt\": e.jet_pt,\n",
    "                \"jet_eta\": e.jet_eta,\n",
    "                \"jet_phi\": e.jet_phi,\n",
    "                \"jet_btag\": e.jet_btag,\n",
    "                \"numbermuon\": e.numbermuon,\n",
    "                \"numberelectron\": e.numberelectron,\n",
    "                \"numberjet\": e.numberjet,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6049da",
   "metadata": {},
   "source": [
    "### Caching the queried datasets with `ServiceX`\n",
    "\n",
    "Using the queries created with `func_adl`, we are using `ServiceX` to read the CMS Open Data files to build cached files with only the specific event information as dictated by the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051eb5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SERVICEX:\n",
    "    \n",
    "    # dummy dataset on which to generate the query\n",
    "    dummy_ds = ServiceXSourceUpROOT(\"cernopendata://dummy\", \"events\", backend_name=\"uproot\")\n",
    "\n",
    "    # tell low-level infrastructure not to contact ServiceX yet, only to\n",
    "    # return the qastle string it would have sent\n",
    "    dummy_ds.return_qastle = True\n",
    "\n",
    "    # create the query\n",
    "    query = get_query(dummy_ds).value()\n",
    "\n",
    "    # now we query the files and create a fileset dictionary containing the\n",
    "    # URLs pointing to the queried files\n",
    "\n",
    "    t0 = time.time()\n",
    "    for process in fileset.keys():\n",
    "        ds = ServiceXDataset(fileset[process]['files'], \n",
    "                             backend_name=\"uproot\", \n",
    "                             ignore_cache=SERVICEX_IGNORE_CACHE)\n",
    "        files = ds.get_data_rootfiles_uri(query, \n",
    "                                          as_signed_url=True,\n",
    "                                          title=process)\n",
    "\n",
    "        \n",
    "        fileset[process][\"files\"] = [f.url for f in files]\n",
    "\n",
    "    print(f\"ServiceX data delivery took {time.time() - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee62067b",
   "metadata": {},
   "source": [
    "### Execute the data delivery pipeline\n",
    "\n",
    "What happens here depends on the flag `USE_SERVICEX`. If set to true, the processor is run on the data previously gathered by ServiceX, then will gather output histograms.\n",
    "\n",
    "When `USE_SERVICEX` is false, the input files need to be processed during this step as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ddf4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_DASK:\n",
    "    executor = processor.DaskExecutor(client=utils.get_client(AF))\n",
    "else:\n",
    "    executor = processor.FuturesExecutor(workers=NUM_CORES)\n",
    "        \n",
    "run = processor.Runner(executor=executor, schema=AGCSchema, savemetrics=True, metadata_cache={}, chunksize=CHUNKSIZE)\n",
    "\n",
    "if USE_SERVICEX:\n",
    "    treename = \"servicex\"\n",
    "    \n",
    "else:\n",
    "    treename = \"events\"\n",
    "    \n",
    "filemeta = run.preprocess(fileset, treename=treename)  # pre-processing\n",
    "\n",
    "t0 = time.monotonic()\n",
    "all_histograms, metrics = run(fileset, treename, processor_instance=TtbarAnalysis(DISABLE_PROCESSING, IO_FILE_PERCENT))  # processing\n",
    "exec_time = time.monotonic() - t0\n",
    "\n",
    "all_histograms = all_histograms[\"hist\"]\n",
    "\n",
    "print(f\"\\nexecution took {exec_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e71c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track metrics\n",
    "dataset_source = \"/data\" if fileset[\"ttbar__nominal\"][\"files\"][0].startswith(\"/data\") else \"https://xrootd-local.unl.edu:1094\" # TODO: xcache support\n",
    "metrics.update({\n",
    "    \"walltime\": exec_time, \n",
    "    \"num_workers\": NUM_CORES, \n",
    "    \"af\": AF_NAME, \n",
    "    \"dataset_source\": dataset_source, \n",
    "    \"use_dask\": USE_DASK, \n",
    "    \"use_servicex\": USE_SERVICEX, \n",
    "    \"systematics\": SYSTEMATICS, \n",
    "    \"n_files_max_per_sample\": N_FILES_MAX_PER_SAMPLE,\n",
    "    \"cores_per_worker\": CORES_PER_WORKER, \n",
    "    \"chunksize\": CHUNKSIZE, \n",
    "    \"disable_processing\": DISABLE_PROCESSING, \n",
    "    \"io_file_percent\": IO_FILE_PERCENT\n",
    "})\n",
    "\n",
    "# save metrics to disk\n",
    "if not os.path.exists(\"metrics\"):\n",
    "    os.makedirs(\"metrics\")\n",
    "timestamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "metric_file_name = f\"metrics/{AF_NAME}-{timestamp}.json\"\n",
    "with open(metric_file_name, \"w\") as f:\n",
    "    f.write(json.dumps(metrics))\n",
    "\n",
    "print(f\"metrics saved as {metric_file_name}\")\n",
    "#print(f\"event rate per worker (full execution time divided by NUM_CORES={NUM_CORES}): {metrics['entries'] / NUM_CORES / exec_time / 1_000:.2f} kHz\")\n",
    "print(f\"event rate per worker (pure processtime): {metrics['entries'] / metrics['processtime'] / 1_000:.2f} kHz\")\n",
    "print(f\"amount of data read: {metrics['bytesread']/1000**2:.2f} MB\")  # likely buggy: https://github.com/CoffeaTeam/coffea/issues/717"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda0dbe8",
   "metadata": {},
   "source": [
    "### Inspecting the produced histograms\n",
    "\n",
    "Let's have a look at the data we obtained.\n",
    "We built histograms in two phase space regions, for multiple physics processes and systematic variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_style()\n",
    "\n",
    "all_histograms[120j::hist.rebin(2), \"4j1b\", :, \"nominal\"].stack(\"process\")[::-1].plot(stack=True, histtype=\"fill\", linewidth=1, edgecolor=\"grey\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\">= 4 jets, 1 b-tag\")\n",
    "plt.xlabel(\"HT [GeV]\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccc7b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_histograms[:, \"4j2b\", :, \"nominal\"].stack(\"process\")[::-1].plot(stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\">= 4 jets, >= 2 b-tags\")\n",
    "plt.xlabel(\"$m_{bjj}$ [Gev]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a301fb0",
   "metadata": {},
   "source": [
    "Our top reconstruction approach ($bjj$ system with largest $p_T$) has worked!\n",
    "\n",
    "Let's also have a look at some systematic variations:\n",
    "- b-tagging, which we implemented as jet-kinematic dependent event weights,\n",
    "- jet energy variations, which vary jet kinematics, resulting in acceptance effects and observable changes.\n",
    "\n",
    "We are making of [UHI](https://uhi.readthedocs.io/) here to re-bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67552ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b-tagging variations\n",
    "all_histograms[120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"nominal\"].plot(label=\"nominal\", linewidth=2)\n",
    "all_histograms[120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"btag_var_0_up\"].plot(label=\"NP 1\", linewidth=2)\n",
    "all_histograms[120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"btag_var_1_up\"].plot(label=\"NP 2\", linewidth=2)\n",
    "all_histograms[120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"btag_var_2_up\"].plot(label=\"NP 3\", linewidth=2)\n",
    "all_histograms[120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"btag_var_3_up\"].plot(label=\"NP 4\", linewidth=2)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"HT [GeV]\")\n",
    "plt.title(\"b-tagging variations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7dc710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jet energy scale variations\n",
    "all_histograms[:, \"4j2b\", \"ttbar\", \"nominal\"].plot(label=\"nominal\", linewidth=2)\n",
    "all_histograms[:, \"4j2b\", \"ttbar\", \"pt_scale_up\"].plot(label=\"scale up\", linewidth=2)\n",
    "all_histograms[:, \"4j2b\", \"ttbar\", \"pt_res_up\"].plot(label=\"resolution up\", linewidth=2)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"$m_{bjj}$ [Gev]\")\n",
    "plt.title(\"Jet energy variations\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f163c19",
   "metadata": {},
   "source": [
    "### Save histograms to disk\n",
    "\n",
    "We'll save everything to disk for subsequent usage.\n",
    "This also builds pseudo-data by combining events from the various simulation setups we have processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76026216",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_histograms(all_histograms, fileset, \"histograms.root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a218b6ba",
   "metadata": {},
   "source": [
    "### Statistical inference\n",
    "\n",
    "A statistical model has been defined in `config.yml`, ready to be used with our output.\n",
    "We will use `cabinetry` to combine all histograms into a `pyhf` workspace and fit the resulting statistical model to the pseudodata we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34991461",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cabinetry.configuration.load(\"cabinetry_config.yml\")\n",
    "cabinetry.templates.collect(config)\n",
    "cabinetry.templates.postprocess(config)  # optional post-processing (e.g. smoothing)\n",
    "ws = cabinetry.workspace.build(config)\n",
    "cabinetry.workspace.save(ws, \"workspace.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbe0f7a",
   "metadata": {},
   "source": [
    "We can inspect the workspace with `pyhf`, or use `pyhf` to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120cbab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyhf inspect workspace.json | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35caf78a",
   "metadata": {},
   "source": [
    "Let's try out what we built: the next cell will perform a maximum likelihood fit of our statistical model to the pseudodata we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea761d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data = cabinetry.model_utils.model_and_data(ws)\n",
    "fit_results = cabinetry.fit.fit(model, data)\n",
    "\n",
    "cabinetry.visualize.pulls(\n",
    "    fit_results, exclude=\"ttbar_norm\", close_figure=True, save_figure=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d68ec73",
   "metadata": {},
   "source": [
    "For this pseudodata, what is the resulting ttbar cross-section divided by the Standard Model prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03029702",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_index = model.config.poi_index\n",
    "print(f\"\\nfit result for ttbar_norm: {fit_results.bestfit[poi_index]:.3f} +/- {fit_results.uncertainty[poi_index]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2401f93",
   "metadata": {},
   "source": [
    "Let's also visualize the model before and after the fit, in both the regions we are using.\n",
    "The binning here corresponds to the binning used for the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b5c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = cabinetry.model_utils.prediction(model)\n",
    "figs = cabinetry.visualize.data_mc(model_prediction, data, close_figure=True)\n",
    "figs[0][\"figure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce43feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[1][\"figure\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce8184",
   "metadata": {},
   "source": [
    "We can see very good post-fit agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbc868",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction_postfit = cabinetry.model_utils.prediction(model, fit_results=fit_results)\n",
    "figs = cabinetry.visualize.data_mc(model_prediction_postfit, data, close_figure=True)\n",
    "figs[0][\"figure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb80070",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[1][\"figure\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3475b8ad",
   "metadata": {},
   "source": [
    "### What is next?\n",
    "\n",
    "Our next goals for this pipeline demonstration are:\n",
    "- making this analysis even **more feature-complete**,\n",
    "- **addressing performance bottlenecks** revealed by this demonstrator,\n",
    "- **collaborating** with you!\n",
    "\n",
    "Please do not hesitate to get in touch if you would like to join the effort, or are interested in re-implementing (pieces of) the pipeline with different tools!\n",
    "\n",
    "Our mailing list is analysis-grand-challenge@iris-hep.org, sign up via the [Google group](https://groups.google.com/a/iris-hep.org/g/analysis-grand-challenge)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
